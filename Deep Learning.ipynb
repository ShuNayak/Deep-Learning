{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious how deep learning and artificial neural networks work, this notebook will help you :)\n",
    "some mathematical jargon for deep learning\n",
    "\n",
    "Gradient Descent : ML technique to find the most optimal set of parameters for a given problem. GD requires the knowledge of, the gradient from your cost function(MSE). GD is mainly like plotting your cost function maybe like error function and mesuring how close the prediction is to the expected results each time. There are chances of local minimum but this is rare and does not often occur.\n",
    "\n",
    "Reverse Mode autodiff: Can compute all the derivatives just by traversing the graph (#outcomes+1) times, Tensorflow uses autodiff to compute its gradient descent.\n",
    "\n",
    "SoftMax: When we have a neural network, we have a bunch of weights that come out of the neural network, how do we make practical use of the output from the neural network? softmax converts the outcome weights into probability for each class.\n",
    "\n",
    "The bilogical inspiration is the human brain, Neurons in the brain are connected to each other. A neuron fires to another neuron when enough of its input signals are activated. Layers of neurons connected yield learning behavior. Billions of neurons connected yield a mind.\n",
    "\n",
    "Neurons in your brain are arranged into many stacks or 'columns' that process the information in parallel. Mini columns of arounfd 100 neurons are organized into larger \"hyper columns\". There are 100 million mini columns in your brain. This is coincidentally how GPU works.\n",
    "\n",
    "The LTU (Linear Threshold Unit) : Add weights to the inputs the weights could be positive or negative. Say the positive weight activates and the negative weight supresses, output is given by the step function. Output is given by a step function, say sum of input weights and product of inputs >=0 then the output is fired else not.\n",
    "\n",
    "The perceptrons: Layers of multiple LTU, a perceptron can learn by reinforcing the weights that lead to correct behavior during training.It slike \"Cells that fire together, wire together\". Bias netron in a perceptron sometimes to add a constant to make things work mathematically.\n",
    "\n",
    "A layer of perceptrons:  This is deep neural network. There is opportunity to reinforce the weights between each connection.\n",
    "\n",
    "A modern deep neural network: Replace step activation function with something better. Apply softmax to output, and use gradient descent during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we train a multilayer perceptron weights? Using a technique called back propagation or more specifically called as gradient descent using reverse mode autodiff. For each training steps:\n",
    "1. Compute the output error\n",
    "2. Compute how much each neuron in the previous hidden layer contributed\n",
    "3. Back propagate that error in a reverse pass [ Take the error that is computed and back propagate it, push it back to the neural network backwards, that way we can distribute that error back to all connections all the way to the inputs, using the weights we are currently using at this training step. We are using the current weights in the neural network to back propagate the error, we can use to error to tweak the weights through gradient descent to arrive at a better model]\n",
    "4. Tweak weights to reduce the error using gradient descent\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Activation Function determines the ouput of a Neuron given the sum of the inputs, also called as rectifier. Tep functions donot work with gradient descent because there is no gradient or slope because it is either on or off. Mathematically they have no useful derivative.\n",
    "\n",
    "AlternativeS:\n",
    "1. Logistic Function\n",
    "2. Hyperbolic Tangent Function\n",
    "3. Exponential Linear Unit (ELU)\n",
    "4. Rectified Linear Unit (ReLu), very common fast to compute and works very well. There is also leaky ReLu and Noisy ReLu, ELU can also sometimes lead to faster learning.\n",
    "\n",
    "Normally ReLu climbs for positive values ( indicating that sum is + it rises) and is the negative y axis (flat) for negative values. In Leaky ReLu the negative values also there is downward slope. ELU is often used and produces faster learning.\n",
    "\n",
    "Optimization functions: There are many variants /optimizers than gradient descent.\n",
    "1. Momentum Optimization: Introduces a momentum term to the descent, so it slows down as things start to flatten and speeds up as slope is steep.\n",
    "2. Nesterov Accelerated Gradient: A small tweak on momentum optimization, it captures the momentum based on the gradient slightly ahead of you, not where you are.\n",
    "3. RMSProp: adaptive learning rate to help point toward the minimum.\n",
    "4. Adaptive moment estimation, momentum + RMSProp combined.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to avoid overfitting? With thousands of weights to tune, overfitting is a problem. You are just fitting to the training data given and not the pattern. Ways to handle it :\n",
    "1. Early stopping - when the performance starts to drop.\n",
    "2. Regularization terms added to the cost function during the training, this is the bias term we discussed.\n",
    "3. Most commonly used method is dropout : randomly ignore 50 % of neurons at each training step. Works surprisingly well, force your model to spread out its learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
