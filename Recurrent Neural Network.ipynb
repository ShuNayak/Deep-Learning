{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN are for sequence of data, this might be a sequence in time , like processing time series data. process a set of data points over time and predict their outcome in future.\n",
    "Example of time series data could be sensor logs where we get sensor data from Internet of things; one example could be for a self driving car. We know where the car has been in the past and we will know about its future trajectories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent neuron look very much similar to the previous neuron except for the loop. As we feed some training data into the neuron, some training data gets fed into it. Some step fucntion is applied , the output is forwarded and also fed back into the recurrent neuronso that the next time data is fed into it, the previous output is also considered in the step function.\n",
    "The more recent behavior has more influence on the output from the current neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN topologies: Using RNNs we can deal with sequence of data and ot just the static snapshots. \n",
    "1. Sequence to Sequence: if we have an input that is a time series of data and the output will also be a timeseries of data. Eg: predicing stock prices in future based on historical trades\n",
    "2. Sequence to vector: taking a sequence of data and provide a snapshot / static data. Eg: words in a sentence to sentiment\n",
    "3. Vector to sequence: taking an image a static vector of information and creating captionsEg:create captions from an image\n",
    "4. Encode-> decoder : that is a sequence into a vector and then back to a sequence. This is machine translation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In RNNs while training, the state from the earlier time steps gets diluted over time. This can be a problem in situations where the earlier state is even more important. Eg: When learning sentence structures, the words in the start matter even more.\n",
    "LSTM cell:(Long short term memory cell)  Maintains separate ideas of both short term and long term states. If you dealing with a sequence of data and we donot want to give any preferential treatment to recent data, then we use a LSTM cell.\n",
    "GRU cell: (Grated recurrent cell) it is a simplified LSTM that performs about as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training RNNs can be really hard, they are very sensitive to topologies and choice of hyperparameters. They are very resource intensive and wrong choice of hyperparameters can lead to a RNN that does not converege at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
